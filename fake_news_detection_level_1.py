# -*- coding: utf-8 -*-
"""Fake_News_Detection_Level_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xVl1FWcCF8Rtv5-s5gY7kJjojZ9B8FL3

#Team 5
#### Name : Milan Dudhatra(015998645)
#### Name : Prit Lakhani(016025646) 
#### Name : Dhruva Gajera(016040934) 
---

# Title : Fake News Detection

---
### Project Track : Algorithm 

---
### Dataset : Dataset contains the detailed information about the particular news i.e. Source of News, Title, image url, etc.

Link: https://github.com/entitize/Fakeddit

---
"""

from google.colab import drive
drive.mount('/content/drive')

#root_path = 'gdrive/My Drive/your_project_folder/'  #change dir to your project folder
root_path = '/content/drive/MyDrive/Dataset'

import os
os.chdir(root_path)

"""## Loading dataset   





"""

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

# Now just move the glove.6B.100d.txt file directly from local folder to your drive folder from table of content file.

"""## Importing libraries"""

import keras
from tensorflow.python.client import device_lib

print(device_lib.list_local_devices())
import numpy as np
import pandas 

import pandas as pd
from collections import defaultdict
import re


import sys
import os

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical

from keras.layers import Embedding
from keras.layers import Dense, Input, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout
from keras.models import Model



#NlTK

from wordcloud import WordCloud,STOPWORDS
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches


#For Model

from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D
from keras.layers import Reshape, Flatten, Dropout, Concatenate
from keras.callbacks import ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from keras.models import Model
from sklearn.model_selection import train_test_split

# Train Test Split
from sklearn.model_selection import train_test_split

MAX_SEQUENCE_LENGTH = 1000
MAX_NB_WORDS = 200000
EMBEDDING_DIM = 100
VALIDATION_SPLIT = 0.2

"""## Change to Project Directory"""

root_path = '/content/drive/MyDrive/Dataset/Project_Fake_News_Detection'
os.chdir(root_path)

"""## Dataset link : https://github.com/entitize/Fakeddit"""

#Give path to file train.tsv from folder Essential
df = pd.read_csv("Essential/train.tsv",sep='\t') # Here train.tsv is the same dataset just upload it to your respective google drive and copy the path here
df.head()

"""## Data cleaning"""

df1=df.iloc[:,[5,7,8,10,12,13,17]]#['num_comments','score','upvote_ratio','clean_title','2_way_label'])
df1=df1.iloc[:,:]
df1.head()

df1.iloc[:,:-1]=df1.iloc[:,:-1].replace(to_replace = np.nan, value ='')

df1.iloc[:,-1]=df1.iloc[:,-1].replace(to_replace = np.nan, value =0)

df1.head()

"""## Tokenizing"""

texts = []
labels = []

for i in range(len(list(df1.clean_title))):
    text = str(df1['clean_title'][i])
    texts.append(text)
    labels.append(df1['2_way_label'][i])
    
tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index

print('Found %s unique tokens.' % len(word_index))

"""## Pad input sequences

"""

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = to_categorical(np.asarray(labels,dtype='int32'),num_classes = 2)
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

from keras.models import Sequential
from keras.layers.convolutional import Conv3D
from keras.layers.convolutional_recurrent import ConvLSTM2D
from tensorflow.keras.layers import BatchNormalization
import numpy as np
from matplotlib import pyplot as plt
from keras.layers import Dense, Embedding, LSTM, GRU

"""## The file that we have downloaded and moved from local session to drive. """

GLOVE_DIR = "" 
embeddings_index = {}
f = open(os.path.join(GLOVE_DIR, '/content/drive/MyDrive/Dataset/glove.6B.100d.txt'), encoding="utf8")

for line in f:
    values = line.split()
    #print(values[1:])
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Total %s word vectors in Glove.' % len(embeddings_index))

embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
        
embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH)

"""## Creating model"""

from keras.callbacks import ModelCheckpoint
from keras import layers

embedding_vecor_length = 32
modell = Sequential()
modell.add(embedding_layer)
modell.add(Dropout(0.2))
modell.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
modell.add(MaxPooling1D(pool_size=2))
modell.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='sigmoid'))
modell.add(MaxPooling1D(pool_size=2))
modell.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))
modell.add(MaxPooling1D(pool_size=2))
modell.add(LSTM(100,activation='relu',recurrent_activation='sigmoid',dropout=0.2, recurrent_dropout=0.2))
modell.add(BatchNormalization())
modell.add(Dense(1024, activation='relu'))
modell.add(layers.Dropout(0.2))
modell.add(Dense(512,  activation='relu'))
modell.add(layers.Dropout(0.2))
modell.add(Dense(256, activation='relu'))
modell.add(layers.Dropout(0.2))
modell.add(Dense(128, activation='relu'))
modell.add(layers.Dropout(0.2))
modell.add(Dense(64, activation='relu'))
modell.add(layers.Dropout(0.2))
modell.add(Dense(2, activation='softmax'))

modell.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(modell.summary())

"""*italicised text*## Saving Model"""

#give path MPFolder save your model in MPFolder
filepath = "/content/drive/MyDrive/Dataset/Project_Fake_News_Detection/MPFolder/model.h5" # Location to save yor model
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
modell.fit(data, labels,validation_split=0.05, epochs=9, batch_size=1024 , callbacks=callbacks_list)

"""## Loading and Training model"""

from keras.models import load_model
filepath = "/content/drive/MyDrive/Dataset/Project_Fake_News_Detection/MPFolder/model.h5"  # Location to get yor model
filepath1 = "/content/drive/MyDrive/Dataset/Project_Fake_News_Detection/MPFolder/fakenewsdetect.h5" # Location to save yor model
checkpoint = ModelCheckpoint(filepath1, monitor='loss', verbose=1, save_best_only=True, mode='min')

new_model = load_model(filepath)
callbacks_list = [checkpoint]

new_model.fit(data, labels,validation_split=0.05, epochs=9, batch_size=1024 , callbacks=callbacks_list)

"""## Ploting Model"""

!pip install keras.utils

from keras.utils.vis_utils import plot_model
plot_model(new_model, to_file='new_model.png')

"""# Prediction On Validation Data """

#Give Path to MPFolder's fakenewsdetection.h5 file
from tensorflow import keras
model = keras.models.load_model('/content/drive/MyDrive/Dataset/Project_Fake_News_Detection/MPFolder/fakenewsdetect.h5')

#Give Path to Essential Folder's validate.tsv file
import pandas as pd
tmps = pd.read_csv('Essential/validate.tsv',sep='\t')
tmps.head()

df1.columns

tmps = tmps.loc[:,['clean_title', 'domain', 'hasImage', 'image_url', 'num_comments','score', '2_way_label']]
tmps

tmps.iloc[:,:-1]=tmps.iloc[:,:-1].replace(to_replace = np.nan, value ='')

tmps.iloc[:,-1]=tmps.iloc[:,-1].replace(to_replace = np.nan, value =0)

texts = []
labels = []

for i in range(len(list(tmps.clean_title))):
    text = str(tmps['clean_title'][i])
    texts.append(text)
    labels.append(tmps['2_way_label'][i])
    
tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index

print('Found %s unique tokens.' % len(word_index))

# Pad input sequences
datatmp = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = to_categorical(np.asarray(labels,dtype='int32'),num_classes = 2)
print('Shape of data tensor:', datatmp.shape)
print('Shape of label tensor:', labels.shape)

pred = model.predict(datatmp)
print(pred)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

target_names = ['False', 'True']

print(classification_report(np.argmax(labels, axis=1), np.argmax(pred, axis=1), target_names=target_names))

import seaborn as sns

cf_matrix = confusion_matrix(np.argmax(labels, axis=1), np.argmax(pred, axis=1))
sns.heatmap(cf_matrix, annot=True)

sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')

accuracy_score(np.argmax(labels, axis=1), np.argmax(pred, axis=1))

result = pd.DataFrame(data = (pred))
result

dpred = pd.DataFrame(data = (pred))
dpred
dpred.columns = ['False','True']
dpred.columns

# dpred['Fake'] = dpred[dpred['False'] >= 0.7]['False']
# dpred['Real'] = dpred[dpred['True'] >= 0.7]['True']
# dpred

dpred['IsFake'] = dpred['False'] > 0.7
dpred
# dpred['Prediction'] = dpred['IsFake'].astype(int32)

"""# Prediction On Test Data"""

#Give Path to Essential Folder's test_public.tsv file
import pandas as pd
tmps = pd.read_csv('Essential/test_public.tsv',sep='\t')
tmps.head()

df1.columns

tmps = tmps.loc[:,['clean_title', 'domain', 'hasImage', 'image_url', 'num_comments','score', '2_way_label']]
tmps

tmps.iloc[:,:-1]=tmps.iloc[:,:-1].replace(to_replace = np.nan, value ='')

tmps.iloc[:,-1]=tmps.iloc[:,-1].replace(to_replace = np.nan, value =0)

texts = []
labels = []

for i in range(len(list(tmps.clean_title))):
    text = str(tmps['clean_title'][i])
    texts.append(text)
    labels.append(tmps['2_way_label'][i])
    
tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index

print('Found %s unique tokens.' % len(word_index))

# Pad input sequences
datatmp = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = to_categorical(np.asarray(labels,dtype='int32'),num_classes = 2)
print('Shape of data tensor:', datatmp.shape)
print('Shape of label tensor:', labels.shape)

pred = model.predict(datatmp)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

target_names = ['False', 'True']

print(classification_report(np.argmax(labels, axis=1), np.argmax(pred, axis=1), target_names=target_names))

import seaborn as sns

cf_matrix = confusion_matrix(np.argmax(labels, axis=1), np.argmax(pred, axis=1))
sns.heatmap(cf_matrix, annot=True)

sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt='.2%', cmap='Blues')

accuracy_score(np.argmax(labels, axis=1), np.argmax(pred, axis=1))

result = pd.DataFrame(data = (pred))
result

dpred = pd.DataFrame(data = (pred))
dpred
dpred.columns = ['False','True']
dpred.columns

# dpred['Fake'] = dpred[dpred['False'] >= 0.7]['False']
# dpred['Real'] = dpred[dpred['True'] >= 0.7]['True']
# dpred

dpred['IsFake'] = dpred['False'] > 0.7
dpred
# dpred['Prediction'] = dpred['IsFake'].astype(int32)

ls = tmps['clean_title'][92443]

ls

dpred['Prediction'] = ""
for i in range(len(dpred)):
  if(dpred['IsFake'][i]==False):
    dpred['Prediction'][i]="Real"
  else:
    dpred['Prediction'][i]="Fake"

dpred

"""# Testing on random data

###We have tried hand picked data by various websites and add it to Testing csv file and get prediction of model. 
"""

# from tensorflow import keras
# model = keras.models.load_model('/content/drive/My Drive/hackerthon1/hackerthon1.h5')

#Give Path to Essential Folder's Testtttting - Testtttting.csv file
import pandas as pd
tmps = pd.read_csv('Essential/Testtttting - Testtttting.csv',sep=',')
tmps.head()

tmps = {
    'clean_title' : ["President-elect Donald J. Trump takes the oath of office as the 45th President of the United States in Washington, Jan. 20, 2017."],
    'domain': ["https://abcnews.go.com/"],
    'hasImage': ["TRUE"],
    'image_url': ["https://s.abcnews.com/images/Politics/GTY-donald-trump-oath-1-jt-170120_16x9_992.jpg"],
    'num_comments': [19],
    'score' : [100],
  }
tmps = pd.DataFrame(data=tmps)

tmps

df1.columns

# tmps = tmps.loc[:,['clean_title', 'domain', 'hasImage', 'image_url', 'num_comments','score', '2_way_label']]
# tmps

tmps.iloc[:,:-1]=tmps.iloc[:,:-1].replace(to_replace = np.nan, value ='')

tmps.iloc[:,-1]=tmps.iloc[:,-1].replace(to_replace = np.nan, value =0)

texts = []
labels = []

for i in range(len(list(tmps.clean_title))):
    text = str(tmps['clean_title'][i])
    texts.append(text)
    # labels.append(tmps['2_way_label'][i])
    
tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index

print('Found %s unique tokens.' % len(word_index))

# Pad input sequences
datatmp = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = to_categorical(np.asarray(labels,dtype='int32'),num_classes = 2)
print('Shape of data tensor:', datatmp.shape)
print('Shape of label tensor:', labels.shape)

pred = model.predict(datatmp)

result = pd.DataFrame(data = (pred))
result

dpred = pd.DataFrame(data = (pred))
dpred
dpred.columns = ['False','True']
dpred.columns

dpred['IsFake'] = dpred['False'] > 0.7
# dpred.columns = ['False','True','IsFake','Prediction']
dpred['Prediction'] = ""
for i in range(len(dpred)):
  if(dpred['IsFake'][i]==False):
    dpred['Prediction'][i]="Real"
  else:
    dpred['Prediction'][i]="Fake"
# dpred['Prediction'] = dpred['IsFake'].astype(int32)

dpred